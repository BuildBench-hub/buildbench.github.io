<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Build-bench</title>
    <link rel="stylesheet" href="assets/styles.css">
</head>
<body>
<section class="hero">
  <div class="hero-header">
    <img src="assets/logo.png" alt="Logo" class="logo" />
      <h1 class="title">Build-bench</h1>
  </div>
  <h2 class="subtitle">Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems</h2>
    <div class="authors">
        <p>
            <span class="author-name">Chenyu Zhao, Shenglin Zhang, Zeshun Huang, Weilin Jin, Yongqian Sun, Dan Pei, <br/> Chaoyun Zhang, Qingwei Lin, Chetan Bansal, Saravan Rajmohan, Minghua Ma</span><br/><br/>
            <span class="author-aff">NanKai University, Microsoft </span>
        </p>
    </div>

    <div class="buttons">
        <a href="https://arxiv.org/abs/2511.00780v1" class="btn">üìÑ Paper</a>
        <a href="https://github.com/zcyyc/Build-bench" class="btn">üì¶ Code</a>
    </div>
</section>

<section class="content_a" id="results">
  <p class="abstract">
    We present Build-bench, an end-to-end benchmark that systematically evaluates the capability of LLMs to repair build failures in cross-ISA settings.
  </p>
  <!-- <h2>Experimental Results</h2> -->
   <div class="figure">
    <img src="assets/fig1.png" class="figure-img" />
    <p class="caption"><strong>Fig. 1.</strong> Comparison of different large language models (LLMs) in cross-ISA build repair tasks. (a) shows the success rates (%) achieved on four migration scenarios (x86_64‚Üíaarch64 (F), x86_64‚Üíaarch64 (P), aarch64‚Üíx86_64 (F), and aarch64‚Üíx86_64 (P)),
where F denotes Full File Generation and P denotes Patch Generation. (b) summarizes the overall success rates across all tasks for each model.
</p>
  </div>
</section>

<section class="content">
  <p>
    Large language models (LLMs) have shown growing potential in software engineering, yet few benchmarks evaluate their ability to repair software during migration across instruction set architectures (ISAs). 
    Cross-ISA migration, such as between <strong>x86_64 and aarch64</strong>, requires handling complex dependencies, heterogeneous toolchains, and long build logs while ensuring executable verification. 
  </p>
  <p>
    To address this challenge, we present <span class="highlight">Build-bench</span>, an end-to-end benchmark that systematically evaluates the capability of LLMs to repair build failures in cross-ISA settings. 
    Build-bench collects <strong>268</strong> real-world failed packages and integrates auxiliary tools including Structure Extraction, File Content Extraction, Content Modification, and Build Verification to support autonomous, tool-augmented reasoning. 
    The repair process operates in an iterative loop where, upon failure, the model receives updated build logs and previous repair outcomes to refine subsequent attempts. 
  </p>
  <p>
    Through a comparative evaluation of <strong>six representative LLMs</strong>, Build-bench reveals that current models achieve a maximum build success rate of <strong>63%</strong> and tool usage patterns differ significantly across models. 
    By coupling real build environments with verifiable outcomes, Build-bench establishes the first architecture-aware benchmark for studying LLM-based software build and repair.
  </p>
</section>

<section class="content" id="workflow">
  <h2>System Workflow</h2>
  <p>
    The overall workflow of Build-bench, illustrated in Fig. 2, consists of three major stages: (1) Input & Diagnosis Context, (2) LLM-driven Repair Process, and (3) Verification & Evaluation.  </p>
</section>

<section class="content_a">
  <div class="figure">
    <img src="assets/framework.png" class="figure-framework" />
    <p class="caption"><strong>Fig. 2.</strong> The automatic cross-ISA repair and build pipeline of Build-bench. 
      If the build fails and the maximum iteration ùëÅ_max = 3 is not reached, the process repeats with the updated build log as well as the previous repair content.
    </p>
    
  </div>
</section>


<section class="content" id="workflow">
<ul class="workflow-list">
  <li>
    <strong>Stage 1:</strong>  
    Build-bench collects essential contextual artifacts from each failed package directory to support diagnosis and repair.
  </li>

  <li>
    <strong>Stage 2:</strong>  
    The second stage leverages an LLM-driven repair module based on the <span class="highlight">Model Context Protocol (MCP)</span>, which allows dynamic interaction between the model and a suite of external tools for information extraction and content modification.
To assess how different editing granularities affect model performance, the repair process produces two experimental variants evaluated in Section 4.5: Full File Generation, where the model regenerates the entire faulty file while preserving its structure and minimal edits, and Patch Generation, where explicit line-level modifications are output in a diff-like
format automatically applied on the relevant file by Build-bench.
  </li>

  <li>
    <strong>Stage 3:</strong>  
    The updated package is rebuilt on the Open Build Service (OBS) to verify whether the repair succeeds. If the rebuild fails and the maximum iteration threshold is not reached, the process repeats with the updated inputs. 
    This iterative workflow enables reproducible, end-to-end evaluation of LLM-based repair performance across heterogeneous ISAs.
  </li>
</ul>

</section>


<section class="content" id="workflow">
  <h2>Cross-ISA Build Tasks</h2>
  <p>
    As shown in Table 1, we evaluate both the effectiveness and efficiency of model-driven repair. The following metrics are adopted in Build-bench:
  </p>
  <ul class="workflow-list">
      <li>
        Build Success Rate: the percentage of packages that are successfully built on the target architecture within N_max iterations.
      </li>
      <li>
        Average Repair Time (min): the average time a package takes until successful build or termination.
      </li>
      <li>
        Average Token Consumption (K): the average number of input and output tokens the model consumes for each package during the entire repair process.
      </li>
    </ul>
  <p>
  This formulation provides a clear and measurable framework for assessing whether LLMs can understand, adapt, and repair software packages in cross-ISA migration scenarios, emphasizing their reasoning depth, contextual utilization, and cost-effectiveness.
  </p>
    
</section>

<section class="content_a">
  
  <div class="figure">
    <p class="caption"><strong>Table. 1.</strong> Performance of LLMs on cross-ISA build failures in both migration directions. 
      For each model, Success indicates the number of packages that are successfully built; Success Rate corresponds to Build Success Rate; Avg Time (min) corresponds to Average Repair Time; Avg Tokens (K) corresponds to Average Token Consumption.
    </p>
    <img src="assets/table2.png" class="figure-framework" />
    
  </div>
</section>

<section class="content">
  <p>
    Overall, the results reveal a trade-off between reasoning depth and efficiency. Models that maintain longer reasoning chains and richer tool interactions tend to achieve higher success rates but consume more time and tokens.
    Conversely, faster models often exhibit insufficient context retention or under-exploration of repair strategies.
  <p>
    Table.2 summarizes the effect of iterative feedback on cumulative repair success across three iterations in Build-bench. 
    Each iteration reuses the latest build log and prior repair output as contextual feedback, allowing the model to refine its reasoning and avoid repeating ineffective edits. 
    Across both migration directions, iterative feedback consistently improves repair outcomes, confirming that LLMs benefit from exposure to updated diagnostic information.
  </p>
</section>

<section class="content_a">
  <div class="figure">
    <p class="caption"><strong>Table.2 </strong> Iteration-wise improvement in build success rate on Build-bench. Iter-1, Iter-2, and Iter-3 denote the cumulative build success rates after the first, second, and third iterations, respectively. Œî(3‚Äì1) represents the improvement between the first and third iterations.
    </p>
    <img src="assets/table3.png" class="figure-framework" />
    
  </div>
</section>

<section class="content" id="workflow">
  <h2>Tool Invocation Behavior Across LLMs</h2>
  <p>
    To further understand the behavioral characteristics of different models during cross-ISA repair, we analyze their tool invocation patterns. 
    The bars indicate the total number of invocations for each tool across all 268 packages (including both migration directions), while the gray line represents the average number of tool calls per iteration, averaged over all repair attempts of each package.
    These findings reveal that LLMs differ not only in their linguistic reasoning capabilities but also in their operational strategies during tool-assisted repair, which is an important factor contributing to their divergent success rates in cross-ISA build repair.
  </p>
</section>

<section class="content_a">
  <div class="figure">
    <img src="assets/tool_usage.png" class="figure-framework" />
    <p class="caption"><strong>Fig. 3. </strong> Comparison of tool invocation behavior across LLMs. The bars represent the total number of invocations for each tool per LLM, while the gray line indicates the average number of tool calls per iteration
    </p>
  </div>
</section>


<section class="content" id="citation">
  <h2>BibTeX citation</h2>

  <pre class="bibtex">
@misc{zhao2025languagemodelscodingassessing,
      title={Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems}, 
      author={Chenyu Zhao and Shenglin Zhang and Zeshun Huang and Weilin Jin and Yongqian Sun and Dan Pei and Chaoyun Zhang and Qingwei Lin and Chetan Bansal and Saravan Rajmohan and Minghua Ma},
      year={2025},
      eprint={2511.00780},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2511.00780}, 
}
  </pre>
</section>


<footer class="footer">
 ¬© 2025 Build-bench
</footer>
</body>
</html>
